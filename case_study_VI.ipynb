{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Case Study: Email AnalyticsDomain: IT Security Firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SpamDetection Notebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.read.option(\"delimiter\",\"#\").csv(\"use_cases/maildir/output.csv\").toDF(\"messageid\",\"date\",\"from_\",\"to_\",\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = raw.withColumn(\"date\",F.trim(F.split(raw.date,\":\")[1])).withColumn(\"from_\",F.trim(F.split(raw.from_,\":\")[1])).withColumn(\"to_\",F.trim(F.split(raw.to_,\":\")[1])).withColumn(\"subject\",F.trim(F.split(raw.subject,\"Subject:\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|           messageid|               date|               from_|                 to_|             subject|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|Message-ID: <2586...|Tue, 16 Oct 2001 10|greg.whalley@enro...|mark.frevert@enro...|FW: Project South...|\n",
      "|Message-ID: <1534...|Fri, 28 Sep 2001 06|greg.whalley@enro...|john.sherriff@enr...|    RE: Confidential|\n",
      "|Message-ID: <2997...| Mon, 5 Nov 2001 15|diana.scholtes@en...|josie.jarnagin@en...|RE: Reminder-Flu ...|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|               from_|count|            avgcount|\n",
      "+--------------------+-----+--------------------+\n",
      "|shelley.corman@en...|  481|                9.25|\n",
      "|lynn.blair@enron.com|  300|   5.769230769230769|\n",
      "|greg.whalley@enro...|   68|  1.3076923076923077|\n",
      "|diana.scholtes@en...|   61|  1.1730769230769231|\n",
      "|martin.cuilla@enr...|   20| 0.38461538461538464|\n",
      "|michelle.cash@enr...|    2|0.038461538461538464|\n",
      "|brad.mckay@enron.com|    1|0.019230769230769232|\n",
      "+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the top 10 high frequency users based on weekly numbers of mails send\n",
    "df1 = df.withColumn(\"week\", F.weekofyear(F.unix_timestamp(df.date,\"EEE, dd MMM yyyy HH\").cast(\"timestamp\")))\n",
    "maxweek = df1.agg(F.max(df1.week)).first()[0]\n",
    "df1.groupBy(\"from_\").count().withColumn(\"avgcount\",F.col(\"count\")/maxweek).sort(F.col(\"avgcount\").desc()).show()\n",
    "                                                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer().setInputCol(\"subject\").setOutputCol(\"words\")\n",
    "transformed = tokenizer.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| keyword|count|\n",
      "+--------+-----+\n",
      "|     re:|  407|\n",
      "|     fw:|  245|\n",
      "|     for|   59|\n",
      "|       -|   57|\n",
      "| meeting|   52|\n",
      "|      of|   39|\n",
      "|      to|   34|\n",
      "|     nng|   29|\n",
      "|      on|   27|\n",
      "|     the|   26|\n",
      "|      tw|   22|\n",
      "|     gas|   20|\n",
      "|     and|   19|\n",
      "|  update|   18|\n",
      "|    2001|   18|\n",
      "| request|   17|\n",
      "|northern|   16|\n",
      "|   lunch|   15|\n",
      "|      in|   15|\n",
      "|     pep|   15|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----+\n",
      "|keyword|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract top 20 keywords from the subject text for both 1) for the top 10 high frequency users and 2) for the non-high frequency users\n",
    "top_users = [v.asDict()[\"from_\"] for v in df1.groupBy(\"from_\").count().sort(F.col(\"count\").desc()).take(10)]\n",
    "topuserdata = transformed.filter(transformed.subject != \"\").filter(transformed.from_.isin(top_users))\n",
    "topuserdata.withColumn(\"keyword\",F.explode(\"words\")).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show(20)\n",
    "otheruserdata = transformed.filter(transformed.subject != \"\").filter(transformed.from_.isin(top_users) == False)\n",
    "otheruserdata.withColumn(\"keyword\",F.explode(\"words\")).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show(20)\n",
    "                                 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract top 10 keywords from subject by identifying removing the common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| keyword|count|\n",
      "+--------+-----+\n",
      "|     re:|  407|\n",
      "|     fw:|  245|\n",
      "|     for|   59|\n",
      "|       -|   57|\n",
      "| meeting|   52|\n",
      "|      of|   39|\n",
      "|      to|   34|\n",
      "|     nng|   29|\n",
      "|      on|   27|\n",
      "|     the|   26|\n",
      "|      tw|   22|\n",
      "|     gas|   20|\n",
      "|     and|   19|\n",
      "|  update|   18|\n",
      "|    2001|   18|\n",
      "| request|   17|\n",
      "|northern|   16|\n",
      "|   lunch|   15|\n",
      "|      in|   15|\n",
      "|     pep|   15|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.filter(cleaned.subject != \"\").withColumn(\"keyword\",F.explode(cleaned.words)).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a new column label to identify new, replied, and forwarded messages\n",
    "df2 = cleaned.withColumn(\"msgtype\",F.when(cleaned.subject.startswith(\"Re:\"),1).otherwise(F.when(cleaned.subject.startswith(\"Fw:\"),2).otherwise(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+----+\n",
      "|week|  0|   1|   2|\n",
      "+----+---+----+----+\n",
      "|  31|  3|null|null|\n",
      "|  34| 34|null|null|\n",
      "|  26| 10|null|null|\n",
      "|  27|  2|null|null|\n",
      "|  44| 40|   2|null|\n",
      "|  12| 34|   1|null|\n",
      "|  47| 24|null|null|\n",
      "|   1| 10|null|null|\n",
      "|  52| 24|null|null|\n",
      "|  13|  9|   1|null|\n",
      "|   6| 14|   6|   1|\n",
      "|   3| 20|   4|null|\n",
      "|  40| 20|null|null|\n",
      "|  48|100|null|null|\n",
      "|   5| 11|   3|null|\n",
      "|  41| 14|null|null|\n",
      "|  43| 12|null|   1|\n",
      "|  37|  2|null|null|\n",
      "|   9| 35|   5|null|\n",
      "|  35| 28|null|null|\n",
      "+----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the trend of the overall mail activity using the pivot table from spark itself\n",
    "df2.groupBy(\"week\").pivot(\"msgtype\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert keywords to feature vector\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "df4 = df2.filter(df2.subject != \"\")\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(df4)\n",
    "featured = cvmodel.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kmeans clustering to create 4 clusters from the extracted keywords\n",
    "from org.apache.spark.ml.clustering import KMeans\n",
    "kmeans = KMeans().setK(4).setSeed(1L)\n",
    "model = kmeans.fit(featured)\n",
    "predictions = model.transform(featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use LDA to generate 4 topics from the extracted keywords\n",
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA().setK(4).setMaxIter(10)\n",
    "model = lda.fit(featured)\n",
    "topics = model.describeTopics(4)\n",
    "topic_indices = topics.select(\"termIndices\").rdd.map(lambda x:x[0][0]).collect()\n",
    "[cvmodel.vocabulary[v] for v in topic_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
