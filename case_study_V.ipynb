{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SpamDetection Notebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.read.option(\"delimiter\",\"\\t\").csv(\"use_cases/SMSSpamCollection\").toDF(\"spam\",\"message\")\n",
    "raw.count()\n",
    "raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|spam|             message|               words|\n",
      "+----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract word\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tockenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "transformed = tockenizer.transform(raw)\n",
    "transformed.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(2)\n",
    "StopWordsRemover().getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|            features|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(13457,[8,12,33,6...|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|(13457,[0,26,307,...|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate features\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(cleaned)\n",
    "featured = cvmodel.transform(cleaned)\n",
    "featured.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(13457,[8,12,33,6...|  0.0|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|(13457,[0,26,307,...|  0.0|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(13457,[2,14,20,3...|  1.0|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(13457,[0,71,83,1...|  0.0|\n",
      "| ham|Nah I don't think...|[nah, i, don't, t...|[nah, don't, thin...|(13457,[36,39,141...|  0.0|\n",
      "|spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(13457,[11,57,62,...|  1.0|\n",
      "| ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|(13457,[11,55,108...|  0.0|\n",
      "| ham|As per your reque...|[as, per, your, r...|[per, request, 'm...|(13457,[130,196,4...|  0.0|\n",
      "|spam|WINNER!! As a val...|[winner!!, as, a,...|[winner!!, valued...|(13457,[1,50,124,...|  1.0|\n",
      "|spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|(13457,[0,1,14,29...|  1.0|\n",
      "| ham|I'm gonna be home...|[i'm, gonna, be, ...|[i'm, gonna, home...|(13457,[5,19,36,4...|  0.0|\n",
      "|spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|(13457,[9,18,40,9...|  1.0|\n",
      "|spam|URGENT! You have ...|[urgent!, you, ha...|[urgent!, 1, week...|(13457,[14,32,50,...|  1.0|\n",
      "| ham|I've been searchi...|[i've, been, sear...|[i've, searching,...|(13457,[42,98,101...|  0.0|\n",
      "| ham|I HAVE A DATE ON ...|[i, have, a, date...|[date, sunday, wi...|(13457,[566,1746,...|  0.0|\n",
      "|spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13457,[32,111,11...|  1.0|\n",
      "| ham|Oh k...i'm watchi...|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13457,[85,222,47...|  0.0|\n",
      "| ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(13457,[0,2,52,13...|  0.0|\n",
      "| ham|Fine if thats th...|[fine, if, thats...|[fine, thats, wa...|(13457,[0,76,107,...|  0.0|\n",
      "|spam|England v Macedon...|[england, v, mace...|[england, v, mace...|(13457,[4,32,35,8...|  1.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert to binary label\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)\n",
    "indexed.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham| &lt;#&gt;  in mc...|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|(13457,[3,7,5193,...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split to train and test\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "training, test = indexed.randomSplit([0.7, 0.3], seed = 123)\n",
    "training.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(13457,[3,7,44,21...|  0.0|       0.0|\n",
      "|(13457,[3,87,117,...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "('Accuracy', 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.5021834061135371)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "model = rf.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ngrams                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go jurong, jurong point,, point, crazy.., crazy.. available, available bugis, bugis n, n great, great world, world la, la e, e buffet..., buffet... cine, cine got, got amore, amore wat...]|\n",
      "|[ok lar..., lar... joking, joking wif, wif u, u oni...]                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(cleaned)\n",
    "ngramDataFrame.select(\"ngrams\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "\n",
    "stopwords = StopWordsRemover().getStopWords()+ [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "#lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, rf])\n",
    "model = pipeline.fit(raw)\n",
    "model.write().overwrite().save(\"use_cases/spam_model4.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_49338d5723284896f832,\n",
       " StopWordsRemover_4119b791bc44870af081,\n",
       " CountVectorizer_4069ba267fc1ec8c5b5c,\n",
       " StringIndexer_460097ce0707a30782e4,\n",
       " RandomForestClassificationModel (uid=rfc_764684cd4958) with 10 trees]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model = PipelineModel.load(\"use_cases/spam_model4.4\")\n",
    "pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|spam|             message|               words|            filtered|            features|label|       rawPrediction|         probability|prediction|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(13457,[8,12,33,6...|  0.0|[8.95971895993946...|[0.89597189599394...|       0.0|\n",
      "| ham|Ok lar... Joking ...|[ok, lar..., joki...|[ok, lar..., joki...|(13457,[0,26,307,...|  0.0|[8.95971895993946...|[0.89597189599394...|       0.0|\n",
      "|spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(13457,[2,14,20,3...|  1.0|[7.09109101771172...|[0.70910910177117...|       0.0|\n",
      "| ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(13457,[0,71,83,1...|  0.0|[8.95971895993946...|[0.89597189599394...|       0.0|\n",
      "| ham|Nah I don't think...|[nah, i, don't, t...|[nah, don't, thin...|(13457,[36,39,141...|  0.0|[8.95971895993946...|[0.89597189599394...|       0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.transform(raw).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
